// Copyright (C) 2023 Intel Corporation
// SPDX-License-Identifier: Apache 2.0
include "device.fbs";
include "memoryManagement.fbs";
include "structure.fbs";
include "actKernel.fbs";

namespace MVCNN;

table Version{
  /// Version of the schema that the graphfile was generated with.
  /// Major Version: Sigificant Architectural Change (This will probably require design documentation and such)
  /// Minor Verson: Regular Release of Graphfile Schema.
  /// Patch Version: Hot-Fixes and Patches if needed.
  majorV: uint;
  minorV: uint;
  patchV: uint;
  /// You can provide a string to pin-point the exact commit hash or tag a graphfile came from.
  /// This can be used for other types of labelling if desired.
  hash: string;

  // The "context" string can be used to give a 'human readable' explanation
  // on how this blob was generated. Please prioritize brevity over comprehensiveness.
  context: string;
}


table Resources{
  // Limitations on the usage of resources for the VPU.
  // The below values are maximum utilization limits, rather than specific instructions
  // e.g. A software layer may have 12 Shaves assigned, but only execute using 3.
  // These decisions are made at the Software library level.

  // These numbers also do not indicate *which* resources, merely the number of them.
  // This is another decision decided by the VPU application.
  // e.g. "Allocate 3 Shaves" does not restrict whether the code runs on Shaves numbers 0,1,2 or numbers 2,5,11

  // ============ Processors ============

  // available_processors: [ResourceMapping];
  // // At the moment, we presume any multiple instances of a process run at the same freq.
  processor_allocation: [ProcessorMapping];
  processor_frequencies: [ProcessorMapping];

  // // ============ Memory ============
  memory_sizes: [MemoryMapping];
  memory_bandwidth: [MemoryRelationshipMapping];  // GB/s

}

enum ExecutionFlag : byte {
  // each of these enums' presence informs how the current schedule is configured.
  // These must be extended with care. They are not for cheap workarounds.
  // Note: Some flags may be incompatible with others.

  // This graphFile was generated yet is inoperable or a untested configuration.
  // The lack of this flag indicates that the compiler has good faith that this is a valid schedule.
  INVALID = 0,

  // This flag signals that the runtime should schedule with a Dynamic Barrier Scheme.
  // If this field is not in the list of execution flags, it is assumed to run with Static Barrier Scheme.
  DynamicBarriers = 1,

  // This field marks that our blob is compatible with HKSwitch
  // ODU offset patching, field is added in to have backward compatibility with
  // current state of runtime code
  IsHKSwitchOutputSymmetric = 2,
}

enum PreProcessColorSpace : uint { DEFAULT, BGR, RGB, NV12, I420 }
enum PreProcessResizeAlgorithm : uint { NO_RESIZE, RESIZE_BILINEAR, RESIZE_AREA }
table preprocessingInfo {
  input_name: string;
  input_format: PreProcessColorSpace;
  output_format: PreProcessColorSpace;
  algorithm: PreProcessResizeAlgorithm;
}

// The DPU HWP payload contents are configurable via this enum.
enum PerfDataMode : byte {
  MODE0,
  MODE1,
  MODE2,
  MODE3
}

enum OVNodeType : int { UNDEFINED, DYNAMIC, BOOLEAN, BF16, F16, F32, F64, I4, I8, I16, I32, I64, U1, U4, U8, U16, U32, U64 }
table OVNode {
  friendly_name: string;
  type: OVNodeType;
  shape: [uint64];
  tensor_names: [string];
  // For results only
  input_name: string;
}

table SummaryHeader{
  version: Version;

  // Unique string to identify one graphfile from another
  identifier: string;

  // The overall network inputs and outputs.
  net_input: [TensorReference];
  net_output: [TensorReference];

  // The amount of VPU Tasks in the network (System-Level)
  task_count: uint;
  // The amount of traditional NN layers in the network (User-Level)
  layer_count: uint;

  options: [ExecutionFlag];

  resources: Resources;
  original_structure: SourceStructure;

  // Tensor references representing inputs and outputs format requested by the end user.
  // Fields are not modified by the compiler and are not in use during the inference runtime.
  // Might be different from net_input/net_output, representing actual inputs and outputs formats.
  in_tensor_desc: [TensorReference];
  out_tensor_desc: [TensorReference];

  // Target device
  device: TargetDevice;

  // Device revision
  device_revision: TargetDeviceRevision;

  // The Activation SHAVE Runtime (Management Kernel) to be used for this schedule
  act_kernel_runtime: ActKernelRuntime;

  // The overall profiling output. 
  profiling_output: [TensorReference];

  // The field stores the info about preprocessing for each input
  pre_process_info: [preprocessingInfo];

  // The compiler expected performance metrics for this schedule
  performance_metrics: PerformanceMetrics;

  // OV 2.0 parameters (inputs) info
  ov_parameters: [OVNode];

  // OV 2.0 results (outputs) info
  ov_results: [OVNode];

  // This mode dictates the data payload written by the DPU hardware profilier (DPU WHP)
  // for given DPU workload.
  // See hwp_cmx_local_slice_base in NCEInvariantFields and workload_id in NCEVariantFields.
  profiling_data_mode: PerfDataMode = MODE0;

  // Indicates whether DMA hardware profiling (DMA HWP) is enabled for this inference
  dma_hwp_enabled : bool = false;

  // This tensor represent DMA HWP base to be set by the runtime in CDMA_HWP_ADR for the context the inference is running in.
  dma_hwp_base : TensorReference;

  // This optional tensor points to 8 byte location within profiling_output
  workpoint_output : TensorReference;
}
